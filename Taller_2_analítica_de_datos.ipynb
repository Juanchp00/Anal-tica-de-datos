{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller 2 analítica de datos ",
      "provenance": [],
      "authorship_tag": "ABX9TyPUDZsSmBhCxse/q0j+Pc6q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanchp00/Anal-tica-de-datos/blob/main/Taller_2_anal%C3%ADtica_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rxMcu0E_2_Y"
      },
      "source": [
        "#Taller 2\n",
        "\n",
        "**Juan José Chamorro Paz**\n",
        "\n",
        "**817513**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPpDEY2AEWN"
      },
      "source": [
        "##Consulta \n",
        "1. Consultar modelo, función de costo y estrategia de optimización de los \n",
        "siguientes clasificadores:\n",
        "\n",
        "  * Naive Bayes\n",
        "  * Linear discriminant analysis\n",
        "  * SGD classifier\n",
        "  * Linear SVC\n",
        "  * SVC con kernel rbf\n",
        "  * Random Forest\n",
        "  * K neighbors classifier\n",
        "  * Logistic Regression (es un clasificador no regresor)\n",
        "\n",
        "\n",
        "2. A partir de la base de datos trabajada en el cuaderno \n",
        "https://github.com/amalvarezme/AnaliticaDatos/blob/master/3_DeteccionClasificacionGH/pruebapeopleclasificacion.ipynb , realizar un análisis comparativo en términos de acierto, classification report, ROC, y AUC de los métodos del punto 1. Utilizar validación cruzada según el cuaderno referido. Discuta los resultados obtenidos y pruebe distintos pipelines que incluyan normalización standard scaler, min max, y sin normalización.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbJTzzoXBNTO"
      },
      "source": [
        "###Punto 1\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKZ-AX8assw"
      },
      "source": [
        "####Naive Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSchHD1jBTsA"
      },
      "source": [
        "se basa en la suposición “ingenua” de Bayes la cual asume independencia condicional entre cada par de características dado el valor de la variable de clase.\n",
        "El teorema de Bayes establece que \n",
        "\n",
        "$$P(y | x_1, ..., x_n)=\\frac{P(y)P(x_1, ..., x_n|y)}{P(x_1, ..., x_n)} $$\n",
        "\n",
        "asumiendo \"ingenuamente\" la independencia condicional: \n",
        "\n",
        "$$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n)=P(x_i|y)$$\n",
        "\n",
        "para todos $i$, por lo que \n",
        "\n",
        "$$P(y | x_1, ..., x_n)=\\frac{P(y)\\prod_{i=1}^n P(x_i|y)}{P(x_1, ..., x_n)} $$\n",
        "\n",
        "Con $P(x_1,...,x_n)$ constante dada la entrada, se puede obtener la siguiente regla de clasificación\n",
        "\n",
        "$$\\hat y = arg max_y P(y)\\prod_{i=1}^n P(x_i|y)$$\n",
        "\n",
        "Usando la aproximación máxima (MAP) para encontrar $P(y)$ y $P(x_i|y)$.\n",
        "Esta clasificación requiere una pequeña cantidad de datos de entrenamiento para estimar los parámetros necesarios, además, suelen ser mucho más rapidos que clasificadores más sofisticados, y cada distribución se puede estimar de forma independiente como una distribución unidimensional debido a el desacoplamiento de las distribuciones de características condicionales de clase, sin embargo, el clasificador \"Naive Bayes\" se cnsidera como un mal estimador, por lo que sus predicciones no son muy tomadas encuenta.\n",
        "\n",
        "Uno de los clasificadores de \"Naive Bayes\" es el Gaussiano el cual implementa una probabilidad Gaussiana para su clasificación.\n",
        "\n",
        "$$P(x_i|y)=\\frac{1}{\\sqrt{2\\pi \\sigma_{y}^2}} exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2})$$\n",
        "\n",
        "Los parametros $\\sigma_y$ y $\\mu_y$ se estiman utilizando la máxima verosimilitud.\n",
        "\n",
        "**Código de importación**\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBjCi_-avEz"
      },
      "source": [
        "####Linear discriminant analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXBoXp06a2HX"
      },
      "source": [
        "Es un clasificador que utiliza la regla de Bayes, este ajusta una densidad gaussiana a cada clase asumiendo que todas las clases comparten la misma matriz de covarianza.\n",
        "\n",
        "Para esta clasificación, se designa un límite de desición lineal que se genera tras el ajuste de las densidades condicionales de cada clase de datos.\n",
        " \n",
        "El clasificador también se puede utilizar para reducir la dimensionalidad de la entrada, proyectándola en las direcciones más discriminatorias, al utilizar el método \"Transform\".\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None)\n",
        "\n",
        "se pueden determinar los siguientes parámetros y atributos.\n",
        " \n",
        "**Parámetros:**\n",
        "* **solver: {‘svd’, ‘lsqr’, ‘eigen’}, default=’svd’** \n",
        "\n",
        "    Puede utilizar 3 valores\n",
        "      * 'svd': Descomposición de valores singulares, es el valor predeterminado y como no calcula la matriz de covarianza, se recomienda para datos con gran cantidad de caracterizticas\n",
        "      * 'lsqr': Solución de mínimos cuadrados. \n",
        "      * 'eigen': Descomposición de valores propios.\n",
        "\n",
        "* **shrinkage: ‘auto’ or float, default=None**\n",
        "\n",
        "  Parámetro de contracción que puede tomar ciertos valores:\n",
        "      * None: No usa ninguna contracción, es el valor predeterminado.\n",
        "      * 'auto': Encogimiento automático usando el lema Ledoit-Wolf.\n",
        "      * float: Parámetro de contracción fijo, puede ser 0 ó 1.\n",
        "\n",
        "* **priors: array-like of shape (n_classes,), default=None**\n",
        "\n",
        "  Muestra las probabilidades previas. su valor predeterminado es '$None$' lo que significa que las proporciones de clase se infieren de los datos de entrenamiento.\n",
        "\n",
        "* **n_component: sint, default=None**\n",
        "\n",
        "  Número de componentes para la reducción de dimensionalidad, su valor predeterminado es '$None$' lo que significa que se establecerá con el mínimo, este parámetro solo afecta al método 'Transform'.\n",
        "\n",
        "* **store_covariance: bool, default=False**\n",
        "\n",
        "  Cuando el parametro es True se calcula explícitamente la matriz de covarianza ponderada dentro de la clase cuando el solucionador sea 'svd'.\n",
        "\n",
        "* **tol: float, default=1.0e-4**\n",
        "\n",
        "  Define el umbral que considera un valor como significativo, solo se usa si el solucionador es 'svd'.\n",
        "\n",
        "* **covariance_estimator: covariance estimator, default=None**\n",
        "\n",
        "  Si el parámetro es '$None$', covariance_estimator se utiliza para estimar las matrices de covarianza en lugar de depender del estimador de covarianza empírico.\n",
        "\n",
        "**Atributos**\n",
        "* **coef_: ndarray of shape (n_features,) or (n_classes, n_features)**\n",
        "\n",
        "  Representa el vector de peso.\n",
        "\n",
        "* **intercept_: ndarray of shape (n_classes,)**\n",
        "\n",
        "  Representa el término de intersección.\n",
        "\n",
        "* **covariance_: array-like of shape (n_features, n_features)**\n",
        "\n",
        "  Corresponde a donde está la matriz de covarianza de las muestras en clase.\n",
        "\n",
        "* **explained_variance_ratio_: ndarray of shape (n_components,)**\n",
        "\n",
        "  Representa el porcentaje de varianza explicada por cada uno de los componentes seleccionados.\n",
        "\n",
        "* **means_: array-like of shape (n_classes, n_features)**\n",
        "\n",
        "  Representan la media de la clase. \n",
        "\n",
        "* **priors_: array-like of shape (n_classes,)**\n",
        "\n",
        "  Representa el a priori de clase.\n",
        "\n",
        "* **scalings_: array-like of shape (rank, n_classes - 1)**\n",
        "\n",
        "  Representa la escala de las entidades en el espacio atravesado por los centroides de clase.\n",
        "\n",
        "* **xbar_: array-like of shape (n_features,)**\n",
        "\n",
        "  Representa la media general. Solo está presente si el solucionador es 'svd'.\n",
        "\n",
        "* **classes_: array-like of shape (n_classes,)**\n",
        "\n",
        "  Representa las etiquetas de clase únicas.\n",
        "\n",
        "\n",
        "Para utilizarlo se debe importar como:\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "Analizar como:\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "Entrenar como:\n",
        "\n",
        "clf.fit()\n",
        "\n",
        "y predecir como:\n",
        "\n",
        "clf.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRFilhl-Yil"
      },
      "source": [
        "####SGD classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpoztp4-kst"
      },
      "source": [
        "Se trata de un clasificador que implementa modelos lineales regularizados con aprendizaje de descenso de gradiente estocástico (SGD), en donde el gadiente de la pérdida se estima en cada muestraa la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente. \n",
        "\n",
        "Para obtener los mejores resultados utilizando el programa de tasa de aprendizaje predeterminado, los datos deben tener una media en cero y una varianza unitaria.\n",
        "\n",
        "La implementación del clasificador funciona con datos representados como matrices densas o dispersas de valores de punto flotante para las características, y el modelo al que se ajusta se puede controlar con el parámetro de pérdida, este, por defecto, se ajusta a una máquina de vectores de soporte lineal (SVM).\n",
        "\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "se pueden determinar los siguientes parámetros y atributos.\n",
        "\n",
        "**Parámetros**:\n",
        "* **losss: tr, default=’hinge’**\n",
        "\n",
        "  Representa la función de pérdida que se utilizará. El valor predeterminado es \"bisagra\", que proporciona una SVM lineal.\n",
        "\n",
        "  Las opciones posibles son 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron' o una pérdida de regresión: 'squared_loss', 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'.\n",
        "\n",
        "  La pérdida 'logarítmica' da una regresión logística, un clasificador probabilístico. 'modified_huber' es otra pérdida suave que aporta tolerancia a valores atípicos, así como estimaciones de probabilidad. 'squared_hinge' es como bisagra pero está penalizado cuadráticamente. 'perceptrón' es la pérdida lineal utilizada por el algoritmo del perceptrón. Las otras pérdidas están diseñadas para la regresión, pero también pueden ser útiles en la clasificación.\n",
        "\n",
        "* **penalty:{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’**\n",
        "\n",
        "  Representa la penalización que se utilizará. El valor predeterminado es 'l2'.\n",
        "\n",
        "* **alpha: float, default=0.0001**\n",
        "\n",
        "  Representa la constante que multiplica el plazo de regularización. Cuanto mayor sea el valor, más fuerte será la regularización.\n",
        "\n",
        "* **l1_ratio: float, default=0.15**\n",
        "\n",
        "  Representa al parámetro de mezcla Elastic Net, con 0 <= l1_ratio <= 1. l1_ratio = 0 corresponde a la penalización L2, l1_ratio = 1 a L1. \n",
        "\n",
        "* **fit_intercept: bool, default=True**\n",
        "\n",
        "  Muestra si la intersección debe estimarse o no. Si es falso, se supone que los datos ya están centrados.\n",
        "\n",
        "* **max_iter: int, default=1000**\n",
        "\n",
        "  Represena al número máximo de pasadas sobre los datos de entrenamiento.\n",
        "\n",
        "* **tol: float, default=1e-3**\n",
        "\n",
        "  Representa el criterio de parada. Si no es '$None$', el entrenamiento se detendrá cuando (loss> best_loss - tol) por n_iter_no_changeépocas consecutivas\n",
        "\n",
        "* **shuffle: bool, default=True**\n",
        "\n",
        "  Muesta si los datos de entrenamiento deben barajarse o no después de cada época.\n",
        "\n",
        "* **verbose: int, default=0**\n",
        "\n",
        "  Representa el nivel de verbosidad.\n",
        "\n",
        "* **epsilon: float, default=0.1**\n",
        "\n",
        "  Epsilon en las funciones de pérdida insensibles a épsilon; solo si losses 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'. Para 'huber', determina el umbral en el que se vuelve menos importante obtener la predicción exactamente correcta. Para los insensibles a épsilon, cualquier diferencia entre la predicción actual y la etiqueta correcta se ignora si es menor que este umbral.\n",
        "\n",
        "* **n_jobs: int, default=None**\n",
        "\n",
        "  Representa el número de CPU que se utilizarán para realizar el cálculo OVA (One Versus All, para problemas de varias clases). '$None$' significa 1 a menos que esté en un joblib.parallel_backendcontexto. -1significa utilizar todos los procesadores. \n",
        "\n",
        "* **random_state: int, RandomState instance, default=None**\n",
        "\n",
        "  Se utiliza para mezclar los datos cuando shufflese establece en True.\n",
        "\n",
        "* **learning_rate: str, default=’optimal’**\n",
        "\n",
        "  Representa el ritmo de aprendizaje, puede ser:\n",
        "      * 'constante': eta = eta0\n",
        "      * 'óptimo': donde t0 es elegido por una heurística propuesta por Leon Bottou.eta = 1.0 / (alpha * (t + t0))\n",
        "      * 'invscaling': eta = eta0 / pow(t, power_t)\n",
        "      * 'adaptativo': eta = eta0, siempre que el entrenamiento siga disminuyendo.\n",
        "\n",
        "* **eta0: double, default=0.0**\n",
        "\n",
        "  Representa la tasa de aprendizaje inicial para los horarios \"constante\", \"invscaling\" o \"adaptativo\" su valor valor predeterminado es 0.0.\n",
        "\n",
        "* **power_t: double, default=0.5**\n",
        "\n",
        "  Representa el exponente de la tasa de aprendizaje de escala inversa, su valor predeterminado es 0,5.\n",
        "\n",
        "* **early_stopping: bool, default=False**\n",
        "\n",
        "  Se utiliza si se debe realizar la parada anticipada para finalizar el entrenamiento cuando la puntuación de validación no mejora. Si se establece en Verdadero, automáticamente apartará una fracción estratificada de los datos de entrenamiento como validación y finalizará el entrenamiento cuando la puntuación de validación devuelta por el scoremétodo no mejore en al menos tol para n_iter_no_change épocas consecutivas.\n",
        "\n",
        "* **validation_fraction: float, default=0.1**\n",
        "\n",
        "  Representa la proporción de datos de entrenamiento que se deben reservar como conjunto de validación para la detención anticipada. Debe estar entre 0 y 1.\n",
        "\n",
        "* **n_iter_no_change: int, default=5**\n",
        "\n",
        "  Representa el número de iteraciones sin mejora que esperar antes de detener el ajuste.\n",
        "\n",
        "* **class_weight: dict, {class_label: weight} or “balanced”, default=None**\n",
        "\n",
        "  Representa los pesos asociados a clases. Si no se da, se supone que todas las clases tienen un peso uno.\n",
        "\n",
        "* **warm_start:bool, default=False**\n",
        "\n",
        "  Si el parámetro se establece en True, reutiliza la solución de la llamada anterior para que encaje como inicialización; de lo contrario, simplemente borra la solución anterior.\n",
        "\n",
        "* **average: bool or int, default=False**\n",
        "\n",
        "  Si el paráetro se establece en Verdadero, se calcula los pesos SGD promediados en todas las actualizaciones y se almacena el resultado en el coef_atributo.\n",
        "\n",
        "\n",
        "**Atributos**:\n",
        "* **coef_: ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features)**\n",
        "\n",
        "  Representa los pesos asignados a las características.\n",
        "\n",
        "* **intercept_: ndarray of shape (1,) if n_classes == 2 else (n_classes,)**\n",
        "\n",
        "  Representa las constantes en función de decisión.\n",
        "\n",
        "* **n_iter_: int**\n",
        "\n",
        "  Representa el número real de iteraciones antes de alcanzar el criterio de parada.\n",
        "\n",
        "* **loss_function_: concrete**\n",
        "* **classes_: array of shape (n_classes,)**\n",
        "* **t_: int**\n",
        "\n",
        "  Representa el número de actualizaciones de peso realizadas durante el entrenamiento.\n",
        "\n",
        "\n",
        "El clasificador se importa mediante:\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "se entrena mediante:\n",
        "\n",
        "funcion.fit(X, Y)\n",
        "\n",
        "y se predice mediante:\n",
        "\n",
        "funcion.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7doQSRG-l0d"
      },
      "source": [
        "####Linear SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QmZRE0Z-qRg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gxl4BZ-qK1"
      },
      "source": [
        "####SVC con kernel rbf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRrVWuv-vHU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhkvjXuO-vdH"
      },
      "source": [
        "####Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJtd2Br-15c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYvNoPTW-2MM"
      },
      "source": [
        "####K neighbors classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az0_g8Pp-40c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bIWGfhD-5VN"
      },
      "source": [
        "####Logistic Regression (es un clasificador no regresor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU8aCvBY-7Bd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4D3rMC8-izW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}