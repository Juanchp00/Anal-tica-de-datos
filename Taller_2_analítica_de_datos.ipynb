{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller 2 analítica de datos ",
      "provenance": [],
      "authorship_tag": "ABX9TyO/D/lD7+YVp/Vmt4QpxM+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanchp00/Anal-tica-de-datos/blob/main/Taller_2_anal%C3%ADtica_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rxMcu0E_2_Y"
      },
      "source": [
        "#Taller 2\n",
        "\n",
        "**Juan José Chamorro Paz**\n",
        "\n",
        "**817513**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPpDEY2AEWN"
      },
      "source": [
        "##Consulta \n",
        "1. Consultar modelo, función de costo y estrategia de optimización de los \n",
        "siguientes clasificadores:\n",
        "\n",
        "  * Naive Bayes\n",
        "  * Linear discriminant analysis\n",
        "  * SGD classifier\n",
        "  * Linear SVC\n",
        "  * SVC con kernel rbf\n",
        "  * Random Forest\n",
        "  * K neighbors classifier\n",
        "  * Logistic Regression (es un clasificador no regresor)\n",
        "\n",
        "\n",
        "2. A partir de la base de datos trabajada en el cuaderno \n",
        "https://github.com/amalvarezme/AnaliticaDatos/blob/master/3_DeteccionClasificacionGH/pruebapeopleclasificacion.ipynb , realizar un análisis comparativo en términos de acierto, classification report, ROC, y AUC de los métodos del punto 1. Utilizar validación cruzada según el cuaderno referido. Discuta los resultados obtenidos y pruebe distintos pipelines que incluyan normalización standard scaler, min max, y sin normalización.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbJTzzoXBNTO"
      },
      "source": [
        "###Punto 1\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKZ-AX8assw"
      },
      "source": [
        "####Naive Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSchHD1jBTsA"
      },
      "source": [
        "se basa en la suposición “ingenua” de Bayes la cual asume independencia condicional entre cada par de características dado el valor de la variable de clase.\n",
        "El teorema de Bayes establece que \n",
        "\n",
        "$$P(y | x_1, ..., x_n)=\\frac{P(y)P(x_1, ..., x_n|y)}{P(x_1, ..., x_n)} $$\n",
        "\n",
        "asumiendo \"ingenuamente\" la independencia condicional: \n",
        "\n",
        "$$P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_n)=P(x_i|y)$$\n",
        "\n",
        "para todos $i$, por lo que \n",
        "\n",
        "$$P(y | x_1, ..., x_n)=\\frac{P(y)\\prod_{i=1}^n P(x_i|y)}{P(x_1, ..., x_n)} $$\n",
        "\n",
        "Con $P(x_1,...,x_n)$ constante dada la entrada, se puede obtener la siguiente regla de clasificación\n",
        "\n",
        "$$\\hat y = arg max_y P(y)\\prod_{i=1}^n P(x_i|y)$$\n",
        "\n",
        "Usando la aproximación máxima (MAP) para encontrar $P(y)$ y $P(x_i|y)$.\n",
        "Esta clasificación requiere una pequeña cantidad de datos de entrenamiento para estimar los parámetros necesarios, además, suelen ser mucho más rapidos que clasificadores más sofisticados, y cada distribución se puede estimar de forma independiente como una distribución unidimensional debido a el desacoplamiento de las distribuciones de características condicionales de clase, sin embargo, el clasificador \"Naive Bayes\" se cnsidera como un mal estimador, por lo que sus predicciones no son muy tomadas encuenta.\n",
        "\n",
        "Uno de los clasificadores de \"Naive Bayes\" es el Gaussiano el cual implementa una probabilidad Gaussiana para su clasificación.\n",
        "\n",
        "$$P(x_i|y)=\\frac{1}{\\sqrt{2\\pi \\sigma_{y}^2}} exp(-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2})$$\n",
        "\n",
        "Los parametros $\\sigma_y$ y $\\mu_y$ se estiman utilizando la máxima verosimilitud.\n",
        "\n",
        "**Código de importación**\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBjCi_-avEz"
      },
      "source": [
        "####Linear discriminant analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXBoXp06a2HX"
      },
      "source": [
        "Es un clasificador que utiliza la regla de Bayes, este ajusta una densidad gaussiana a cada clase asumiendo que todas las clases comparten la misma matriz de covarianza.\n",
        "\n",
        "Para esta clasificación, se designa un límite de desición lineal que se genera tras el ajuste de las densidades condicionales de cada clase de datos.\n",
        " \n",
        "El clasificador también se puede utilizar para reducir la dimensionalidad de la entrada, proyectándola en las direcciones más discriminatorias, al utilizar el método \"Transform\".\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None)\n",
        "\n",
        "se pueden determinar los siguientes parámetros y atributos.\n",
        " \n",
        "**Parámetros:**\n",
        "* **solver: {‘svd’, ‘lsqr’, ‘eigen’}, default=’svd’** \n",
        "\n",
        "    Puede utilizar 3 valores\n",
        "      * 'svd': Descomposición de valores singulares, es el valor predeterminado y como no calcula la matriz de covarianza, se recomienda para datos con gran cantidad de caracterizticas\n",
        "      * 'lsqr': Solución de mínimos cuadrados. \n",
        "      * 'eigen': Descomposición de valores propios.\n",
        "\n",
        "* **shrinkage: ‘auto’ or float, default=None**\n",
        "\n",
        "  Parámetro de contracción que puede tomar ciertos valores:\n",
        "      * None: No usa ninguna contracción, es el valor predeterminado.\n",
        "      * 'auto': Encogimiento automático usando el lema Ledoit-Wolf.\n",
        "      * float: Parámetro de contracción fijo, puede ser 0 ó 1.\n",
        "\n",
        "* **priors: array-like of shape (n_classes,), default=None**\n",
        "\n",
        "  Muestra las probabilidades previas. su valor predeterminado es '$None$' lo que significa que las proporciones de clase se infieren de los datos de entrenamiento.\n",
        "\n",
        "* **n_component: sint, default=None**\n",
        "\n",
        "  Número de componentes para la reducción de dimensionalidad, su valor predeterminado es '$None$' lo que significa que se establecerá con el mínimo, este parámetro solo afecta al método 'Transform'.\n",
        "\n",
        "* **store_covariance: bool, default=False**\n",
        "\n",
        "  Cuando el parametro es True se calcula explícitamente la matriz de covarianza ponderada dentro de la clase cuando el solucionador sea 'svd'.\n",
        "\n",
        "* **tol: float, default=1.0e-4**\n",
        "\n",
        "  Define el umbral que considera un valor como significativo, solo se usa si el solucionador es 'svd'.\n",
        "\n",
        "* **covariance_estimator: covariance estimator, default=None**\n",
        "\n",
        "  Si el parámetro es '$None$', covariance_estimator se utiliza para estimar las matrices de covarianza en lugar de depender del estimador de covarianza empírico.\n",
        "\n",
        "**Atributos**\n",
        "* **coef_: ndarray of shape (n_features,) or (n_classes, n_features)**\n",
        "\n",
        "  Representa el vector de peso.\n",
        "\n",
        "* **intercept_: ndarray of shape (n_classes,)**\n",
        "\n",
        "  Representa el término de intersección.\n",
        "\n",
        "* **covariance_: array-like of shape (n_features, n_features)**\n",
        "\n",
        "  Corresponde a donde está la matriz de covarianza de las muestras en clase.\n",
        "\n",
        "* **explained_variance_ratio_: ndarray of shape (n_components,)**\n",
        "\n",
        "  Representa el porcentaje de varianza explicada por cada uno de los componentes seleccionados.\n",
        "\n",
        "* **means_: array-like of shape (n_classes, n_features)**\n",
        "\n",
        "  Representan la media de la clase. \n",
        "\n",
        "* **priors_: array-like of shape (n_classes,)**\n",
        "\n",
        "  Representa el a priori de clase.\n",
        "\n",
        "* **scalings_: array-like of shape (rank, n_classes - 1)**\n",
        "\n",
        "  Representa la escala de las entidades en el espacio atravesado por los centroides de clase.\n",
        "\n",
        "* **xbar_: array-like of shape (n_features,)**\n",
        "\n",
        "  Representa la media general. Solo está presente si el solucionador es 'svd'.\n",
        "\n",
        "* **classes_: array-like of shape (n_classes,)**\n",
        "\n",
        "  Representa las etiquetas de clase únicas.\n",
        "\n",
        "\n",
        "Para utilizarlo se debe importar como:\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "Analizar como:\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "Entrenar como:\n",
        "\n",
        "clf.fit()\n",
        "\n",
        "y predecir como:\n",
        "\n",
        "clf.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRFilhl-Yil"
      },
      "source": [
        "####SGD classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpoztp4-kst"
      },
      "source": [
        "Se trata de un clasificador que implementa modelos lineales regularizados con aprendizaje de descenso de gradiente estocástico (SGD), en donde el gadiente de la pérdida se estima en cada muestraa la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente. \n",
        "\n",
        "Para obtener los mejores resultados utilizando el programa de tasa de aprendizaje predeterminado, los datos deben tener una media en cero y una varianza unitaria.\n",
        "\n",
        "La implementación del clasificador funciona con datos representados como matrices densas o dispersas de valores de punto flotante para las características, y el modelo al que se ajusta se puede controlar con el parámetro de pérdida, este, por defecto, se ajusta a una máquina de vectores de soporte lineal (SVM).\n",
        "\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "se pueden determinar los siguientes parámetros y atributos.\n",
        "\n",
        "**Parámetros**:\n",
        "* **losss: tr, default=’hinge’**\n",
        "\n",
        "  Representa la función de pérdida que se utilizará. El valor predeterminado es \"bisagra\", que proporciona una SVM lineal.\n",
        "\n",
        "  Las opciones posibles son 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron' o una pérdida de regresión: 'squared_loss', 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'.\n",
        "\n",
        "  La pérdida 'logarítmica' da una regresión logística, un clasificador probabilístico. 'modified_huber' es otra pérdida suave que aporta tolerancia a valores atípicos, así como estimaciones de probabilidad. 'squared_hinge' es como bisagra pero está penalizado cuadráticamente. 'perceptrón' es la pérdida lineal utilizada por el algoritmo del perceptrón. Las otras pérdidas están diseñadas para la regresión, pero también pueden ser útiles en la clasificación.\n",
        "\n",
        "* **penalty:{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’**\n",
        "\n",
        "  Representa la penalización que se utilizará. El valor predeterminado es 'l2'.\n",
        "\n",
        "* **alpha: float, default=0.0001**\n",
        "\n",
        "  Representa la constante que multiplica el plazo de regularización. Cuanto mayor sea el valor, más fuerte será la regularización.\n",
        "\n",
        "* **l1_ratio: float, default=0.15**\n",
        "\n",
        "  Representa al parámetro de mezcla Elastic Net, con 0 <= l1_ratio <= 1. l1_ratio = 0 corresponde a la penalización L2, l1_ratio = 1 a L1. \n",
        "\n",
        "* **fit_intercept: bool, default=True**\n",
        "\n",
        "  Muestra si la intersección debe estimarse o no. Si es falso, se supone que los datos ya están centrados.\n",
        "\n",
        "* **max_iter: int, default=1000**\n",
        "\n",
        "  Represena al número máximo de pasadas sobre los datos de entrenamiento.\n",
        "\n",
        "* **tol: float, default=1e-3**\n",
        "\n",
        "  Representa el criterio de parada. Si no es '$None$', el entrenamiento se detendrá cuando (loss> best_loss - tol) por n_iter_no_changeépocas consecutivas\n",
        "\n",
        "* **shuffle: bool, default=True**\n",
        "\n",
        "  Muesta si los datos de entrenamiento deben barajarse o no después de cada época.\n",
        "\n",
        "* **verbose: int, default=0**\n",
        "\n",
        "  Representa el nivel de verbosidad.\n",
        "\n",
        "* **epsilon: float, default=0.1**\n",
        "\n",
        "  Epsilon en las funciones de pérdida insensibles a épsilon; solo si losses 'huber', 'epsilon_insensitive' o 'squared_epsilon_insensitive'. Para 'huber', determina el umbral en el que se vuelve menos importante obtener la predicción exactamente correcta. Para los insensibles a épsilon, cualquier diferencia entre la predicción actual y la etiqueta correcta se ignora si es menor que este umbral.\n",
        "\n",
        "* **n_jobs: int, default=None**\n",
        "\n",
        "  Representa el número de CPU que se utilizarán para realizar el cálculo OVA (One Versus All, para problemas de varias clases). '$None$' significa 1 a menos que esté en un joblib.parallel_backendcontexto. -1significa utilizar todos los procesadores. \n",
        "\n",
        "* **random_state: int, RandomState instance, default=None**\n",
        "\n",
        "  Se utiliza para mezclar los datos cuando shufflese establece en True.\n",
        "\n",
        "* **learning_rate: str, default=’optimal’**\n",
        "\n",
        "  Representa el ritmo de aprendizaje, puede ser:\n",
        "      * 'constante': eta = eta0\n",
        "      * 'óptimo': donde t0 es elegido por una heurística propuesta por Leon Bottou.eta = 1.0 / (alpha * (t + t0))\n",
        "      * 'invscaling': eta = eta0 / pow(t, power_t)\n",
        "      * 'adaptativo': eta = eta0, siempre que el entrenamiento siga disminuyendo.\n",
        "\n",
        "* **eta0: double, default=0.0**\n",
        "\n",
        "  Representa la tasa de aprendizaje inicial para los horarios \"constante\", \"invscaling\" o \"adaptativo\" su valor valor predeterminado es 0.0.\n",
        "\n",
        "* **power_t: double, default=0.5**\n",
        "\n",
        "  Representa el exponente de la tasa de aprendizaje de escala inversa, su valor predeterminado es 0,5.\n",
        "\n",
        "* **early_stopping: bool, default=False**\n",
        "\n",
        "  Se utiliza si se debe realizar la parada anticipada para finalizar el entrenamiento cuando la puntuación de validación no mejora. Si se establece en Verdadero, automáticamente apartará una fracción estratificada de los datos de entrenamiento como validación y finalizará el entrenamiento cuando la puntuación de validación devuelta por el scoremétodo no mejore en al menos tol para n_iter_no_change épocas consecutivas.\n",
        "\n",
        "* **validation_fraction: float, default=0.1**\n",
        "\n",
        "  Representa la proporción de datos de entrenamiento que se deben reservar como conjunto de validación para la detención anticipada. Debe estar entre 0 y 1.\n",
        "\n",
        "* **n_iter_no_change: int, default=5**\n",
        "\n",
        "  Representa el número de iteraciones sin mejora que esperar antes de detener el ajuste.\n",
        "\n",
        "* **class_weight: dict, {class_label: weight} or “balanced”, default=None**\n",
        "\n",
        "  Representa los pesos asociados a clases. Si no se da, se supone que todas las clases tienen un peso uno.\n",
        "\n",
        "* **warm_start:bool, default=False**\n",
        "\n",
        "  Si el parámetro se establece en True, reutiliza la solución de la llamada anterior para que encaje como inicialización; de lo contrario, simplemente borra la solución anterior.\n",
        "\n",
        "* **average: bool or int, default=False**\n",
        "\n",
        "  Si el paráetro se establece en Verdadero, se calcula los pesos SGD promediados en todas las actualizaciones y se almacena el resultado en el coef_atributo.\n",
        "\n",
        "\n",
        "**Atributos**:\n",
        "* **coef_: ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features)**\n",
        "\n",
        "  Representa los pesos asignados a las características.\n",
        "\n",
        "* **intercept_: ndarray of shape (1,) if n_classes == 2 else (n_classes,)**\n",
        "\n",
        "  Representa las constantes en función de decisión.\n",
        "\n",
        "* **n_iter_: int**\n",
        "\n",
        "  Representa el número real de iteraciones antes de alcanzar el criterio de parada.\n",
        "\n",
        "* **loss_function_: concrete**\n",
        "* **classes_: array of shape (n_classes,)**\n",
        "* **t_: int**\n",
        "\n",
        "  Representa el número de actualizaciones de peso realizadas durante el entrenamiento.\n",
        "\n",
        "\n",
        "El clasificador se importa mediante:\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "se entrena mediante:\n",
        "\n",
        "funcion.fit(X, Y)\n",
        "\n",
        "y se predice mediante:\n",
        "\n",
        "funcion.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7doQSRG-l0d"
      },
      "source": [
        "####Linear SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QmZRE0Z-qRg"
      },
      "source": [
        "Se trata de una clasificación lineal de vectores, la cual funciona igual que el SVC kernel cuando el parámetro $kernel=lineal$, sin embargo, a diferencia de este, tiene mayor flexibilidad en la elección de penalizaciones, funciones de pérdida, y escala mejor a un gran número de muestras.\n",
        "\n",
        "Además, esta clase admite entradas densas y dispersas y el soporte multiclase se maneja de acuerdo con un esquema de uno contra el resto.\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
        "\n",
        "Se pueden determinar los siguientes parámetros y atributos.\n",
        "\n",
        "**Parámetros**\n",
        "\n",
        "  * **penalty: {‘l1’, ‘l2’}, default=’l2’**\n",
        "\n",
        "  especifica la norma que se utilizará en la penalización. la penalización ‘L2’ es la predeterminada para esta clase. \n",
        "\n",
        "  * **loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’**\n",
        "\n",
        "  Especifica la función de pérdida. ‘hinge’ es la pérdida predeterminado de SVM, mientras que ‘squared_hinge’ es el cuadrado de la perdida 'hinge'. No se admite la combinacion de la penalidad 'L1' y la pérdida 'hinge'.\n",
        "\n",
        "  * **dual: bool, default=True**\n",
        "\n",
        "  Selecciona el algoritmo para resolver el problema de optimizacion primario o dual. Se usa preferiblemente $dual=False$ cuando $n_muestras > n_caracteristicas$.\n",
        "\n",
        "  * **tol: float, default=1e-4**\n",
        "\n",
        "  Representa la tolerancia a los críticos de parada.\n",
        "\n",
        "  * **C: float, default=1.0**\n",
        "\n",
        "  Representa el parámetro de regularización. La fuerza de regularización es inversamente proporcional al parámetro C. Debe ser estrictamente positiva.\n",
        "\n",
        "  * **multi_class: {‘ovr’, ‘crammer_singer’}, default=’ovr’**\n",
        "\n",
        "  Determina la estrategia multiclases si contiene mas de 2 clases. El \"ovr\" entrena n_clases uno contra el resto de clasificadores, mientras que, \"crammer_singer\" optimiza un objetivo conjunto de todas las clases. si se escoje \"crammer_singer\", la opción de perdida, penalización y dual se ignoran.\n",
        "\n",
        "  * **fit_intercept: bool, default=True**\n",
        "\n",
        "  Calcula la intercepción del modelo. Si se establece como $false$, se espera que los datos esten centrados y no se calculará ningúna intersección.\n",
        "\n",
        "  * **intercept_scaling: float, default=1**\n",
        "\n",
        "  Cuando 'self.fit_intercept' esta establecido como $True$, la instancia del vector x se conviete en [x, self.intercept_scaling]. \n",
        "\n",
        "  * **class_weight: dict or ‘balanced’, default=None**\n",
        "\n",
        "  Establece el parámetro C de la clase i en 'class_weight[i]*C' para SVC. si no se da, se supone que todas las clases tienen peso unitario. El modo “balanced”utiliza los valores de y para ajustar automaticamente los pesos de forma inversamente proporcional a las frequencias de la clase en los datos de entrada como n_muestras / (n_clases * np.bincount(y)).\n",
        "\n",
        "  * **verbose: int, default=0**\n",
        "\n",
        "  Habilita la salida 'verbose'. Esta configuración arovecha la configuración del tiempo de ejecución de pre-proceso en liblinear, la cual, si se encuentra habilitada, puede que no trabaje adecuadamente en un contexto multiproceso.\n",
        "\n",
        "  * **random_state: int, RandomState instance or None, default=None**\n",
        "\n",
        "  Controla la generacion de números 'pseudo aleatorios' para mezclar los datos para el descenso de cordenadas dual (si dual=True). Cuando dual=False la implementación subyacente de LinearSVC no es aleatoria y 'random_state' no tiene efecto en los resultados.\n",
        "\n",
        "  * **max_iter: int, default=1000**\n",
        "\n",
        " Representa el máximo número de interacciones para ejecutar.\n",
        "\n",
        "**Atributos**\n",
        "\n",
        "  * **coef_: ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features)**\n",
        "\n",
        "  Corresponde a los pesos asignados a las caracteristicas.\n",
        "\n",
        "  * **intercept_: ndarray of shape (1,) if n_classes == 2 else (n_classes,)**\n",
        "\n",
        "  Represeta a las constantes en función de desición.\n",
        "\n",
        "  * **classes_: ndarray of shape (n_classes,)**\n",
        "\n",
        "  Representa las etiquetas de clases únicas.\n",
        "\n",
        "  * **n_iter_: int**\n",
        "\n",
        "  Representa al número máximo de iteraciones que se ejecutan en todas las clases.\n",
        "\n",
        "Para importar el clasificador se utiliza:\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "se entrena mediante:\n",
        "\n",
        "funcion.fit(X, Y)\n",
        "\n",
        "y se predice mediante:\n",
        "\n",
        "funcion.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gxl4BZ-qK1"
      },
      "source": [
        "####SVC con kernel rbf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRrVWuv-vHU"
      },
      "source": [
        "Se trata de un clasificador de vectores de soporte C, cuya implementación se basa en libsvm. A sy¡u vez, el tiempo de ajuste se escala es al menos cuadráticamente con el número de muestras y puede resultar poco práctico más allá de decenas de miles de muestras.\n",
        "\n",
        "El soporte multiclase de el clasificador se maneja de acuerdo con un esquema de uno contra uno.\n",
        "\n",
        "En su clase:\n",
        "\n",
        "class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
        "\n",
        "Se pueden determinar los siguientes parámetros y atributos.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "  * **C: float, default=1.0**\n",
        "  \n",
        "  Parámetro de regularización. La fuerza de la regularización es inversamente proporcional a C. Debe ser estrictamente positiva. La penalización es una penalización de 12 al cuadrado.\n",
        "\n",
        "  * **kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’**\n",
        "\n",
        "  Especifica el tipo de kernel que se utilizará en el algoritmo. Debe ser 'lineal', 'poli', 'rbf', 'sigmoide', 'precalculado' o invocable. Si no se proporciona ninguno, se utilizará 'rbf'. Si se proporciona un invocable, se utiliza para precalcular la matriz del núcleo a partir de matrices de datos; esa matriz debe ser una matriz de formas .(n_muestras, n_muestras)\n",
        "\n",
        "  * **degree: int, default=3**\n",
        "\n",
        "  Grado de la función del núcleo polinomial ('poli'). Es ignorado por todos los demás núcleos.\n",
        "\n",
        "  * **gamma: {‘scale’, ‘auto’} or float, default=’scale’**\n",
        "\n",
        " Coeficiente de kernel para 'rbf', 'poli' y 'sigmoide'.\n",
        "\n",
        "  Si gamma='scale' (por defecto) entonces usa 1 / (n_caracteristicas * X.var ()) como valor de gamma,\n",
        "\n",
        "  Si gamma= ‘auto’, usa 1 / n_caracteristicas.\n",
        "\n",
        "  * **coef0: float, default=0.0**\n",
        "\n",
        "  Término independiente en función del kernel. Solo es significativo en 'poli' y 'sigmoide'.\n",
        "\n",
        "  * **shrinking: bool, default=True**\n",
        "\n",
        "  Usa la heurística de encogimiento.\n",
        "\n",
        "  * **probability: bool, default=False**\n",
        "\n",
        "  Habilita estimaciones de probabilidad, debe estar habilitado antes de llamar fit.\n",
        "\n",
        "  * **tol: float, default=1e-3**\n",
        "\n",
        "  Tolerancia al criterio de parada.\n",
        "\n",
        "  * **cache_size: float, default=200**\n",
        "\n",
        "  Especifica el tamaño del cache de kernel en (MB).\n",
        "\n",
        "  * **class_weight: dict or ‘balanced’, default=None**\n",
        "\n",
        "  Establece el parámetro C de la clase i en 'class_weight[i]*C' para SVC. si no se da, se supone que todas las clases tienen peso unitario. El modo “balanced”utiliza los valores de y para ajustar automaticamente los pesos de forma inversamente proporcional a las frequencias de la clase en los datos de entrada como n_muestras / (n_clases * np.bincount(y)).\n",
        "\n",
        "  * **verbose: bool, default=False**\n",
        "\n",
        "  Habilita la salida 'verbose'. Esta configuración arovecha la configuración del tiempo de ejecución de pre-proceso en liblinear, la cual, si se encuentra habilitada, puede que no trabaje adecuadamente en un contexto multiproceso.\n",
        "\n",
        "  * **max_iter: int, default=-1**\n",
        "\n",
        "  Límite estricto de iteraciones dentro del solucionador, si es -1, significa que no hay límite.\n",
        "\n",
        "  * **decision_function_shape: {‘ovo’, ‘ovr’}, default=’ovr’**\n",
        "\n",
        "  Se utiliza para devolver una función de decisión uno contra el resto ('ovr') de la forma (n_muestras, n_clases) como todos los demás clasificadores, o la función de decisión original uno contra uno ('ovo') de libsvm que tiene la forma (n_muestras , n_clases * (n_clases - 1) / 2). Sin embargo, la función uno contra uno ('ovo') siempre se usa como estrategia de clases múltiples. El parámetro se ignora para la clasificación binaria.\n",
        "\n",
        "  * **break_ties: bool, default=False**\n",
        "\n",
        "  Si es $True$, 'decision_function_shape' sera 'ovr'y se tommara el número de clases mayor a 2, para este caso, la predicción romperá los lazos de acuerdo con los valores de confianza de 'decision_function', De lo contrario, se devuelve la primera clase entre las clases empatadas.\n",
        "\n",
        "  * **random_state: int, RandomState instance or None, default=None**\n",
        "\n",
        "  Controla la generación de números pseudoaleatorios para mezclar los datos para estimaciones de probabilidad, cuando es $False$ ignora la probabilidad.\n",
        "\n",
        "**Attributes**\n",
        "\n",
        "  * **class_weight_: ndarray of shape (n_classes,)**\n",
        "\n",
        "  Multiplicadores del parámetro C para cada clase. Calculado en base al class_weightparámetro.\n",
        "\n",
        "  * **classes_: ndarray of shape (n_classes,)**\n",
        "\n",
        "  Etiquetas de clases.\n",
        "\n",
        "  * **coef_: ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)**\n",
        "\n",
        " Pesos asignados a las características, solo está disponible en el caso de un kernel lineal.\n",
        "\n",
        "  * **dual_coef_: ndarray of shape (n_classes -1, n_SV)**\n",
        "\n",
        " Coeficientes duales del vector de soporte en la función de decisión, multiplicados por sus objetivos. Para multiclases, coeficiente para todos los clasificadores 1 vs 1.\n",
        "\n",
        "  * **fit_status_: int**\n",
        "\n",
        "  0 si esta correctamente instalado, y 1 en otros casos.\n",
        "\n",
        "  * **intercept_: ndarray of shape (n_classes * (n_classes - 1) / 2,)**\n",
        "\n",
        "  Constantes en la función de decisión.\n",
        "\n",
        "  * **support_: ndarray of shape (n_SV)**\n",
        "\n",
        "  Indice de vectores de soporte.\n",
        "\n",
        "  * **support_vectors_: ndarray of shape (n_SV, n_features)**\n",
        "\n",
        "  Vectores de soorte.\n",
        "\n",
        "  * **n_support_: ndarray of shape (n_classes,), dtype=int32**\n",
        "\n",
        "  Número de soporte de vectores para cada clase.\n",
        "\n",
        "  * **probA_: ndarray of shape (n_classes * (n_classes - 1) / 2)**\n",
        "  * **probB_: ndarray of shape (n_classes * (n_classes - 1) / 2)**\n",
        "\n",
        "  Si probabilidad es $True$, corresponde a los parámetros aprendidos en la escala de 'Platt' para producir estimaciones de probabilidad a partir de valores de decisión. Si probabilidad es $False$, corresponde a una matriz vacia. \n",
        "  \n",
        "  * **shape_fit_:tuple of int of shape (n_dimensions_of_X,)**\n",
        "\n",
        "  Dimensiones de matriz del vector de entenamiento X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhkvjXuO-vdH"
      },
      "source": [
        "####Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJtd2Br-15c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYvNoPTW-2MM"
      },
      "source": [
        "####K neighbors classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az0_g8Pp-40c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bIWGfhD-5VN"
      },
      "source": [
        "####Logistic Regression (es un clasificador no regresor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU8aCvBY-7Bd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4D3rMC8-izW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}